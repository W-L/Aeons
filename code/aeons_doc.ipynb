{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Aeons documentation\n",
    "\n",
    "There are 3 different modes:\n",
    "\n",
    "- Aeons_dummy\n",
    "    - creates random graph structures with bubbles and tips\n",
    "    - this was used to generate the initial graphs and other structures but does not use bloom filters yet\n",
    "- Aeons sim\n",
    "    - uses longreadsim to generate reads\n",
    "    - uses bloom filter and generates the graph from that\n",
    "- Aeons run\n",
    "    - growing script for in silico runs\n",
    "    - first place that used read mapping and updating of the graph with new reads\n",
    "\n",
    "\n",
    "# class structure\n",
    "\n",
    "## Bloom\n",
    "\n",
    "a class to hold the bloom filters for an assembly. It actually holds two filter for kmers and k+1-mers that are used\n",
    "for the score calculation (paths through a node). It also keeps track of all kmers observed so far and holds a set\n",
    "of all new kmers that can be used to generate new edge lists and edge weights.\n",
    "\n",
    "\n",
    "## Assembly\n",
    "\n",
    "takes a bloom object and does an initial construction of a dbg. Can then use a new batch of reads to update that dbg\n",
    "with new edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "have to make a decision whether I want to go on using graph-tool to maintain an actual graph or just have it implicit\n",
    "in the bloom filter instead. Would require rebuilding the graph at each batch to calculate the metrics.\n",
    "\n",
    "Maybe stick with the memory costly approach for now until it gets limiting?\n",
    "\n",
    "efficiency idea:\n",
    "instead of saving all observed kmers, save one per read and then recurse into the read?\n",
    "\n",
    "\n",
    "note about implicit graph from bloom:\n",
    "- here edge vs node centric makes a big difference. If we save the counts of edges, we reconstruct the nodes.\n",
    "if we save the counts of nodes, we need to search the neightbors to find the edges, which seems much worse\n",
    "\n",
    "\n",
    "Problem with graph construction:\n",
    "when not skipping the creation of edges that are already present,\n",
    "we end up with two components that are genome sized. But when skipping them, we get N/2 dis-jointed components\n",
    "\n",
    "For now we will end up with everything duplicated. i.e. each kmer leads to two edges, the additional is from the\n",
    "reverse complement. But we reduce the size of the observed kmers by only adding new ones\n",
    " if their reverse complement is not already in there\n",
    "\n",
    "IDEA:\n",
    "if we keep both directions separate, we could have a directed graph which might simplify things.\n",
    "E.g. the hashimoto would be bigger but much more sparse probably.\n",
    "\n",
    "\n",
    "\n",
    "## The problem of statis DBGs\n",
    "\n",
    "Using the data structures we have at the moment, we need to fully reconstruct the dbg whenever we want to update it.\n",
    "This is not good enough since we need to update the graph in sublinear time whenever we have new reads. And we should\n",
    "not need to rebuild it every time.\n",
    "\n",
    "maybe we can still use bloom filters, which can be semi-dynamic by coupling them to a better graph storing structure\n",
    "or we use a hashed adjacency table instead?\n",
    "\n",
    "# MPHF and adjacency list with size n * alphabet\n",
    "maybe with a cuckoo hash to make it possible to extend?\n",
    "\n",
    "if we use a MPHF, we can still use gt as well. gt uses an adjacency list in C++ afterall,\n",
    "which should be efficient enough? And if we have integer mappings to kmers,\n",
    "we should be able to update that in an easier way as well. Then all we need from that adjacency list is a way to\n",
    "transform it into the hashimoto\n",
    "\n",
    "nothing that is available seems to be suitable for what I need:\n",
    "- dynamic minimal perfect hash that can map kmers to indices for an adjacency matrix\n",
    "- BBHash can be used to generate MPHF, but we need to know all kmers in advance\n",
    "- CTB wrote a python binding a few months ago that includes a BBHashTable (pybbhash),\n",
    "    which allows the storage of an index for each kmer\n",
    "    but again, only useful if you know all kmers in advance\n",
    "- crawford FDBG is an implementation of Bazzoguy, but only supports limited dynamic operations (still uses an MPHF and\n",
    "  maps inserted kmers in a much less efficient way. I.e. it expects a very small number of dynamic operations\n",
    "- bloom filter trie by Holley requires complete traversal, making it a bit useless for lots of dynamic updating\n",
    "\n",
    "\n",
    "# Idea for dynamic dBG\n",
    "\n",
    "Build it from multiple components:\n",
    "- a bloom filter that holds the counts for each kmer\n",
    "- an adjacency matrix that describes the graph (sparse matrix)\n",
    "- some hash function that maps the kmers to indices of the matrix\n",
    "  use khmer and hash compression with simple modulo and don't care too much about collisions for now\n",
    "\n",
    "- save all observed kmers in a file, and to avoid the need for a reversible hash function,\n",
    "  at the end we run all kmers through the hash function and check which ones point to the correct indices?\n",
    "\n",
    "\n",
    "# SparseGraph\n",
    "\n",
    "building and updating the graph with a sparse matrix works now\n",
    "\n",
    "also visualisation with gt by looping through entries of matrix and creating edges\n",
    "\n",
    "can observe the false positive edges from hash collisions very nicely if setting the size of the matrix too small\n",
    "- maybe implement a different hash func at some point?\n",
    "\n",
    "or just make the matrix bigger. Not using lil consumes much much less memory, So I can have many more possible indices\n",
    "to distribute my nodes to.\n",
    "\n",
    "\n",
    "# updating scores\n",
    "\n",
    "vectorised the calculation of scores\n",
    "\n",
    "now works with the visualisation as well. Had to sort the arrays by the order of how the edges were added by gt\n",
    "\n",
    "# read length distribution\n",
    "\n",
    "put together a new class (LengthDist) that initialises the prior read length distribution.\n",
    "keeps track of all observed read lengths with a cheap array of unint16\n",
    "and can update the distribution and CCL\n",
    "\n",
    "approximation still needs to be done after we figured out how to calculate U efficiently\n",
    "\n",
    "\n",
    "\n",
    "# updating the benefit U\n",
    "\n",
    "to get consecutive scores from each node, we would use the transition matrix up to the lth power\n",
    "but since the graph is bidirectional we need to use the non-backtracking operator instead\n",
    "which needs a bit more calculation to arrive at transition probabilities\n",
    "we calculate both the probability of transitioning to a node and arriving at that node\n",
    "the first is done with hashimoto turned into a probability matrix\n",
    "and the second with compl. cml. distribution of read lengths\n",
    "Another point to consider are dead-ends. Because ccl can not extend further at these positions,\n",
    "they will have a biased, lowered benefit. Therefore we add some absorbing triangles\n",
    "to the graph that will propagate ccl at dead-ends\n",
    "\n",
    "first order of operations: add the absorbers at component ends\n",
    "absorbers are added and can be visualised as well\n",
    "\n",
    "rewrote the functions to calculate incidence and hashimoto\n",
    "these now work on adjacency matrices instead of on gt graph objects\n",
    "they should be much more readable now as well\n",
    "\n",
    "benefit gets saved as (U - S_mu) because we do not need them separately\n",
    "\n",
    "bif efficiency improvement by not storing the arrival scores and big matrix but as rowsums from the beginning\n",
    "since that is what they are used for in the end. That way we avoid adding 2 sparse matrices while changing the\n",
    "sparsity, which is apparently quite costly. Now its just an addition of numpy arrays instead\n",
    "\n",
    "\n",
    "## path verification as part of calc U\n",
    "\n",
    "there is this subroutine that checks which moves are actually possible in the hashimoto\n",
    "from the observed paths (k+1mers). Instead of rebuilding the k+1mers and checking the bloom filter,\n",
    "which we can not do anymore without the kmers, we also build and update the k+1mer graph and check the edges in there.\n",
    "Remember: the edges in that graph correspond to the paths in the kmer graph.\n",
    "\n",
    "\n",
    "Implemented construction, updating and viz of the p1 graph. and stared at some kmer and k+1mer graphs for verification\n",
    "For the construction we don't actually use `k+1[:1]` and `k+1[-1:]` but `k+1[:2]` and `k+1[:-2]`, since they express\n",
    "basically the same thing but hash to the same value as in the kmer graph! So we can easily index into\n",
    "the k+1 graph for path verification instead of the ridiculous rebuilding of kmers.\n",
    "\n",
    "rewrote the path elimination to use the adjacency of the k+1mer graphs. This is now mostly done with indexing\n",
    "and no calculations needed\n",
    "rewrote the probabilitisation, normalisation and filtering of the matrices\n",
    "checked everything with an example graph that had a little tip. Two of the paths were eliminated and probabilities were\n",
    "calculated properly!\n",
    "\n",
    "benefit is now stored in a sparse matrix as well. makes it easy to work with and assign it to the respective edges\n",
    "\n",
    "adapted a plotting function to show the benefit - seems to work very well so far!!\n",
    "\n",
    "\n",
    "# entropy of the dirichlet\n",
    "\n",
    "does the implementation of the entropy really reflect what I want?\n",
    "it gives the entropy of the dirichlet and not the dirichlet-multinomial\n",
    "\n",
    "found an r package from the strimmer lab that implements some estimators of the dirichlet entropy that seem\n",
    "to make more sense on first experiments. Does not seems to do any better though, since it never reduces further\n",
    "as soon as more than 1 path has been observed. Which the other measure actually does.\n",
    "\n",
    "So maybe the initial one works well enough? I'll move on with that for now and check later how it performs\n",
    "\n",
    "implemented a score array and an update function that uses that array and calculates only for the patterns that\n",
    "are needed to be calculated (either too many paths or coverage too high)\n",
    "\n",
    "\n",
    "# find strategy function\n",
    "\n",
    "adapted to the sparse graph way. All required arrays are trimmed down to the bare essentials\n",
    "Works just the same as the version for BR\n",
    "\n",
    "strategy can be plotted just like the benefit. By mapping the strategy value to the edge weights.\n",
    "For this we are temporarily storing the 0s of the strategy explicitly in the sparse matrix.\n",
    "Should be changed later on to save some memory\n",
    "\n",
    "\n",
    "# GFA\n",
    "\n",
    "created a new class that takes care of writing the gfa (graph) file\n",
    "incrementally appends to a separate segments and links file, then concatenates the two to create a GFA\n",
    "\n",
    "\n",
    "\n",
    "# Mapping reads to GFA\n",
    "\n",
    "There is still the possibility to not map at all but just decompose into kmers and figure things out from there\n",
    "\n",
    "we also never have to map the full reads because we don't care about coverage really. So we can always just map the\n",
    "truncated reads, but track the read lengths for time calculations\n",
    "\n",
    "mapping works, just had to make some longer reads\n",
    "\n",
    "# decision process\n",
    "\n",
    "adapted to new implementation - we now grab the first to node numbers to index into the strategy matrix\n",
    "first two node numbers such that we get the first transitioned edge!\n",
    "\n",
    "also added a new class that pulls everything together\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}