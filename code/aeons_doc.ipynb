{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Aeons documentation\n",
    "\n",
    "There are 3 different modes:\n",
    "\n",
    "- Aeons_dummy\n",
    "    - creates random graph structures with bubbles and tips\n",
    "    - this was used to generate the initial graphs and other structures but does not use bloom filters yet\n",
    "- Aeons sim\n",
    "    - uses longreadsim to generate reads\n",
    "    - uses bloom filter and generates the graph from that\n",
    "- Aeons run\n",
    "    - growing script for in silico runs\n",
    "    - first place that used read mapping and updating of the graph with new reads\n",
    "\n",
    "\n",
    "# class structure\n",
    "\n",
    "## Bloom\n",
    "\n",
    "a class to hold the bloom filters for an assembly. It actually holds two filter for kmers and k+1-mers that are used\n",
    "for the score calculation (paths through a node). It also keeps track of all kmers observed so far and holds a set\n",
    "of all new kmers that can be used to generate new edge lists and edge weights.\n",
    "\n",
    "\n",
    "## Assembly\n",
    "\n",
    "takes a bloom object and does an initial construction of a dbg. Can then use a new batch of reads to update that dbg\n",
    "with new edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "have to make a decision whether I want to go on using graph-tool to maintain an actual graph or just have it implicit\n",
    "in the bloom filter instead. Would require rebuilding the graph at each batch to calculate the metrics.\n",
    "\n",
    "Maybe stick with the memory costly approach for now until it gets limiting?\n",
    "\n",
    "efficiency idea:\n",
    "instead of saving all observed kmers, save one per read and then recurse into the read?\n",
    "\n",
    "\n",
    "note about implicit graph from bloom:\n",
    "- here edge vs node centric makes a big difference. If we save the counts of edges, we reconstruct the nodes.\n",
    "if we save the counts of nodes, we need to search the neightbors to find the edges, which seems much worse\n",
    "\n",
    "\n",
    "Problem with graph construction:\n",
    "when not skipping the creation of edges that are already present,\n",
    "we end up with two components that are genome sized. But when skipping them, we get N/2 dis-jointed components\n",
    "\n",
    "For now we will end up with everything duplicated. i.e. each kmer leads to two edges, the additional is from the\n",
    "reverse complement. But we reduce the size of the observed kmers by only adding new ones\n",
    " if their reverse complement is not already in there\n",
    "\n",
    "IDEA:\n",
    "if we keep both directions separate, we could have a directed graph which might simplify things.\n",
    "E.g. the hashimoto would be bigger but much more sparse probably.\n",
    "\n",
    "\n",
    "\n",
    "## The problem of statis DBGs\n",
    "\n",
    "Using the data structures we have at the moment, we need to fully reconstruct the dbg whenever we want to update it.\n",
    "This is not good enough since we need to update the graph in sublinear time whenever we have new reads. And we should\n",
    "not need to rebuild it every time.\n",
    "\n",
    "maybe we can still use bloom filters, which can be semi-dynamic by coupling them to a better graph storing structure\n",
    "or we use a hashed adjacency table instead?\n",
    "\n",
    "# MPHF and adjacency list with size n * alphabet\n",
    "maybe with a cuckoo hash to make it possible to extend?\n",
    "\n",
    "if we use a MPHF, we can still use gt as well. gt uses an adjacency list in C++ afterall,\n",
    "which should be efficient enough? And if we have integer mappings to kmers,\n",
    "we should be able to update that in an easier way as well. Then all we need from that adjacency list is a way to\n",
    "transform it into the hashimoto\n",
    "\n",
    "nothing that is available seems to be suitable for what I need:\n",
    "- dynamic minimal perfect hash that can map kmers to indices for an adjacency matrix\n",
    "- BBHash can be used to generate MPHF, but we need to know all kmers in advance\n",
    "- CTB wrote a python binding a few months ago that includes a BBHashTable (pybbhash),\n",
    "    which allows the storage of an index for each kmer\n",
    "    but again, only useful if you know all kmers in advance\n",
    "- crawford FDBG is an implementation of Bazzoguy, but only supports limited dynamic operations (still uses an MPHF and\n",
    "  maps inserted kmers in a much less efficient way. I.e. it expects a very small number of dynamic operations\n",
    "- bloom filter trie by Holley requires complete traversal, making it a bit useless for lots of dynamic updating\n",
    "\n",
    "\n",
    "# Idea for dynamic dBG\n",
    "\n",
    "Build it from multiple components:\n",
    "- a bloom filter that holds the counts for each kmer\n",
    "- an adjacency matrix that describes the graph (sparse matrix)\n",
    "- some hash function that maps the kmers to indices of the matrix\n",
    "  use khmer and hash compression with simple modulo and don't care too much about collisions for now\n",
    "\n",
    "- save all observed kmers in a file, and to avoid the need for a reversible hash function,\n",
    "  at the end we run all kmers through the hash function and check which ones point to the correct indices?\n",
    "\n",
    "\n",
    "# SparseGraph\n",
    "\n",
    "building and updating the graph with a sparse matrix works now\n",
    "\n",
    "also visualisation with gt by looping through entries of matrix and creating edges\n",
    "\n",
    "can observe the false positive edges from hash collisions very nicely if setting the size of the matrix too small\n",
    "- maybe implement a different hash func at some point?\n",
    "\n",
    "or just make the matrix bigger. Not using lil consumes much much less memory, So I can have many more possible indices\n",
    "to distribute my nodes to.\n",
    "\n",
    "\n",
    "# updating scores\n",
    "\n",
    "vectorised the calculation of scores\n",
    "\n",
    "now works with the visualisation as well. Had to sort the arrays by the order of how the edges were added by gt\n",
    "\n",
    "# read length distribution\n",
    "\n",
    "put together a new class (LengthDist) that initialises the prior read length distribution.\n",
    "keeps track of all observed read lengths with a cheap array of unint16\n",
    "and can update the distribution and CCL\n",
    "\n",
    "approximation still needs to be done after we figured out how to calculate U efficiently\n",
    "\n",
    "\n",
    "\n",
    "# updating the benefit U\n",
    "\n",
    "to get consecutive scores from each node, we would use the transition matrix up to the lth power\n",
    "but since the graph is bidirectional we need to use the non-backtracking operator instead\n",
    "which needs a bit more calculation to arrive at transition probabilities\n",
    "we calculate both the probability of transitioning to a node and arriving at that node\n",
    "the first is done with hashimoto turned into a probability matrix\n",
    "and the second with compl. cml. distribution of read lengths\n",
    "Another point to consider are dead-ends. Because ccl can not extend further at these positions,\n",
    "they will have a biased, lowered benefit. Therefore we add some absorbing triangles\n",
    "to the graph that will propagate ccl at dead-ends\n",
    "\n",
    "first order of operations: add the absorbers at component ends\n",
    "absorbers are added and can be visualised as well\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}